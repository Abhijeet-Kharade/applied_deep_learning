{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>Total</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>318</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>405</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>525</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>625</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>309</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #                   Name Type 1  Type 2  Total  HP  Attack  Defense  \\\n",
       "0  1              Bulbasaur  Grass  Poison    318  45      49       49   \n",
       "1  2                Ivysaur  Grass  Poison    405  60      62       63   \n",
       "2  3               Venusaur  Grass  Poison    525  80      82       83   \n",
       "3  3  VenusaurMega Venusaur  Grass  Poison    625  80     100      123   \n",
       "4  4             Charmander   Fire     NaN    309  39      52       43   \n",
       "\n",
       "   Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0       65       65     45           1      False  \n",
       "1       80       80     60           1      False  \n",
       "2      100      100     80           1      False  \n",
       "3      122      120     80           1      False  \n",
       "4       60       50     65           1      False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in some text to use\n",
    "poke_df = pd.read_csv('pokemon/Pokemon.csv')\n",
    "poke_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulbasaur Grass<EOS>\n"
     ]
    }
   ],
   "source": [
    "def strip_non_ascii(some_string):\n",
    "    return ''.join([c for c in some_string if c in string.printable])\n",
    "\n",
    "pokemons = [\"{} {}<EOS>\".format(df_row[1]['Name'], df_row[1]['Type 1']) for df_row in poke_df.iterrows()]\n",
    "pokemons = [strip_non_ascii(pokemon) for pokemon in pokemons]\n",
    "print(pokemons[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_iter(pokemon_names):\n",
    "    inp = pokemon_names[:-1] # all but last\n",
    "    targ = pokemon_names[1:] # all but first\n",
    "    \n",
    "    return inp, targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is 800 pokemon long and there are 100 unique characters.\n"
     ]
    }
   ],
   "source": [
    "# need to get all of the possible characters that the source uses\n",
    "chars = string.printable\n",
    "\n",
    "data_size, vocab_size = len(pokemons), len(chars)\n",
    "print('Text is', data_size, 'pokemon long and there are', vocab_size, 'unique characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dictionaries to convert from characters to index and from index back to characters\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some hyperparameters for our network\n",
    "hidden_size = 256\n",
    "seq_length = 50\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN in tensorflow\n",
    "\n",
    "In this notebook we'll work in Tensorflow directly. I would recommend getting familiar with how neural networks work by using our previous examples and then once you feel comfortable with Keras and all of the high level concepts move into Tensorflow. \n",
    "\n",
    "Tensorflow does give us a few helper functions to facilitate the construction of neural networks, but mostly we will be building lots of things from scratch. The one thing that we definitely don't want to do is calculate the backward pass for our training steps, luckily this is something that Tensorflow will do for us. \n",
    "\n",
    "In this example we will create a GRU recurrent neural network to use in our character level RNN. The steps for creating this network from scratch will be:\n",
    "\n",
    "* Initialize all of our weight matrices. Setup their sizes and fill with random numbers\n",
    "* Define the calculations that our network must carry out\n",
    "\n",
    "A GRU cell is basically a change in the way that the hidden state is calculated for a recurrent neural network. So to begin we'll start with a vanilla recurrent neural network and show how we can create one using the two steps above. \n",
    "\n",
    "### Vanilla RNN\n",
    "\n",
    "The calculations for a recurrent neural network look like the following:\n",
    "\n",
    "![rnn](images/rnn.png)\n",
    "\n",
    "In order to create that we need to set up three matrices and two bias vectors. The specify the calculations in exactly the same way. \n",
    "\n",
    "```python\n",
    "Uh = tf.get_variable(\"Uh\", [input_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "Wh = tf.get_variable(\"Wh\", [hidden_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "Vy = tf.get_variable(\"Vy\", [hidden_size, vocab_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "bh  = tf.get_variable(\"bh\", [hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "by  = tf.get_variable(\"by\", [output_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "hs_t = tf.tanh(tf.matmul(xs_t, Uh) + tf.matmul(hs_t, Wh) + bh)\n",
    "ys_t = tf.nn.softmax(tf.matmul(hs_t, Vy) + by)\n",
    "```\n",
    "\n",
    "Simple enough. Input_size and output_size will change depending on the properties of our data. Hidden_size is a hyperparameter that we can set to anything that we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up the place holders for our computational graph\n",
    "inputs = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name='input')\n",
    "targets = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name='targets')\n",
    "init_state = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name='state')\n",
    "\n",
    "# create an initializer to init our weight matricies\n",
    "init = tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up our recurrent neural network and define the functions\n",
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "    # hidden state at time t \n",
    "    hs_t = init_state\n",
    "    # list for output character predictions\n",
    "    ys = []\n",
    "    for t, xs_t in enumerate(tf.split(inputs, 1, axis=0)):\n",
    "        if t > 0: scope.reuse_variables()\n",
    "            \n",
    "        Uh = tf.get_variable(\"Uh\", [vocab_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "        Wh = tf.get_variable(\"Wh\", [hidden_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "        Vy = tf.get_variable(\"Vy\", [hidden_size, vocab_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "        bh  = tf.get_variable(\"bh\", [hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "        by  = tf.get_variable(\"by\", [vocab_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "        hs_t = tf.tanh(tf.matmul(xs_t, Uh) + tf.matmul(hs_t, Wh) + bh)\n",
    "        ys_t = tf.matmul(hs_t, Vy) + by\n",
    "        # add the predicted character to the output list\n",
    "        ys.append(ys_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-c370d4e1dcbb>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need to keep track of our hidden states\n",
    "h_0 = hs_t\n",
    "# apply the softmax output to the last output of our list\n",
    "output_softmax = tf.nn.softmax(ys[-1])\n",
    "\n",
    "# get all of the output characters together\n",
    "outputs = tf.concat(ys, axis=0)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))\n",
    "\n",
    "# optimization algorithm\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
    "grads = optimizer.compute_gradients(loss)\n",
    "\n",
    "# clip the gradients\n",
    "grad_clipping = tf.constant(5.0, name='grad_clipping')\n",
    "clipped_grads = []\n",
    "for grad, var in grads:\n",
    "    clipped_grad = tf.clip_by_value(grad, -grad_clipping, grad_clipping)\n",
    "    clipped_grads.append((clipped_grad, var))\n",
    "    \n",
    "# update the weights with gradient descent\n",
    "updates = optimizer.apply_gradients(clipped_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, p: 0, loss: 4.614536\n",
      "----\n",
      " \tN'sX8EEo%\t22oF*s:0?o(iH-Vg\f",
      "SH>\f",
      "wg7N-w;goHHI\f",
      "g%HH  \n",
      "----\n",
      "\n",
      "iter: 1000, p: 50000, loss: 2.694511\n",
      "----\n",
      " EOS>yEon un<EgaltEOS>S>hugglehff ureon \n",
      "----\n",
      "\n",
      "iter: 2000, p: 100000, loss: 1.822942\n",
      "----\n",
      " er<ElEh<ErfugotsttEuouougg llodugloEOEOS>OSo oustW \n",
      "----\n",
      "\n",
      "iter: 3000, p: 150000, loss: 1.243107\n",
      "----\n",
      " tomatrI<Ermongtoc<EOmatQo<Elir<Egtltor \n",
      "----\n",
      "\n",
      "iter: 4000, p: 200000, loss: 1.963445\n",
      "----\n",
      "  Was<EOSEOS>ak a*<EOSisQu RoHtroatro-Fluma`EOSe(eo \n",
      "----\n",
      "\n",
      "iter: 5000, p: 250000, loss: 1.849594\n",
      "----\n",
      " unmatEOcerHonngllcBunu]Dungo3><EOStt:<EOSw<EnOS>Do \n",
      "----\n",
      "\n",
      "iter: 6000, p: 300000, loss: 1.987277\n",
      "----\n",
      " ic \n",
      "----\n",
      "\n",
      "iter: 7000, p: 350000, loss: 1.572449\n",
      "----\n",
      " game<EOShtHororarE|WirHaVure.M<ErellCgjilMe \n",
      "----\n",
      "\n",
      "iter: 8000, p: 400000, loss: 1.663521\n",
      "----\n",
      " er;r<EOSht=2sscole \n",
      "----\n",
      "\n",
      "iter: 9000, p: 450000, loss: 1.586512\n",
      "----\n",
      " OS>s^uxc<ElStfor.-wr<ElMer= \\leorude WarmH:Grm Lux \n",
      "----\n",
      "\n",
      "iter: 10000, p: 500000, loss: 1.666707\n",
      "----\n",
      " upir.8EOStPon \n",
      "----\n",
      "\n",
      "iter: 11000, p: 550000, loss: 2.241349\n",
      "----\n",
      " OS>s^oic \n",
      "----\n",
      "\n",
      "iter: 12000, p: 600000, loss: 2.174579\n",
      "----\n",
      " ri;`2 (umawFi>ZXMuouudt#ZeaduH<EOiiuRo \\8.<EOSot4g \n",
      "----\n",
      "\n",
      "iter: 13000, p: 650000, loss: 1.494554\n",
      "----\n",
      " asystE;zluHe \n",
      "----\n",
      "\n",
      "iter: 14000, p: 700000, loss: 1.698544\n",
      "----\n",
      " eedug\th WroDufg\\lQuugutMCre LuDa eS|Darr.Z9<Ele.t\n",
      " \n",
      "----\n",
      "\n",
      "iter: 15000, p: 750000, loss: 2.115486\n",
      "----\n",
      " >BuirZjcc \n",
      "----\n",
      "\n",
      "iter: 16000, p: 800000, loss: 2.867780\n",
      "----\n",
      " ocDrogJ- S>LangHZ>XaieZKWarmiIcEOS>G0aie5Jwm S>i>F \n",
      "----\n",
      "\n",
      "iter: 17000, p: 850000, loss: 2.374629\n",
      "----\n",
      " iry HEOS>ireejaHlJ-retHG29irpFGriryueNir% ,iFilZJ< \n",
      "----\n",
      "\n",
      "iter: 18000, p: 900000, loss: 2.178457\n",
      "----\n",
      " erm pc0%rmMat?nabr<EOtHtr \n",
      "----\n",
      "\n",
      "iter: 19000, p: 950000, loss: 1.763007\n",
      "----\n",
      " yatrmEZXctHiter<E%Mer%edKaterougat. WarrMWar<El<El \n",
      "----\n",
      "\n",
      "iter: 20000, p: 1000000, loss: 1.722098\n",
      "----\n",
      " arff E%hlr.ffr<EOSct0ass%oc<EfsMer.<EllMe}rmpC0Uc: \n",
      "----\n",
      "\n",
      "iter: 21000, p: 1050000, loss: 1.931991\n",
      "----\n",
      " >EOm Y+om 'HD;?a Hewomat>jMark<Ex HoJXooer.w;mae<E \n",
      "----\n",
      "\n",
      "iter: 22000, p: 1100000, loss: 1.659230\n",
      "----\n",
      " maff W2 wr<EColtHowollo.Y r<EOSw 0StwlowiSlZA. No\t \n",
      "----\n",
      "\n",
      "iter: 23000, p: 1150000, loss: 1.627223\n",
      "----\n",
      " ar. Xcjas<EOSY 5HeSeotrJ> EOS>c<EOS2ooriwe<EleHoRo \n",
      "----\n",
      "\n",
      "iter: 24000, p: 1200000, loss: 1.886218\n",
      "----\n",
      " ormph.WerrHDuxMelHu<EOSHuJanHHoc \n",
      "----\n",
      "\n",
      "iter: 25000, p: 1250000, loss: 1.860337\n",
      "----\n",
      " ermrt^%erriuxy 5. Euxy2Me(Watrr<EX YrL>Y}eleour<E: \n",
      "----\n",
      "\n",
      "iter: 26000, p: 1300000, loss: 2.142987\n",
      "----\n",
      " stirg\tJririt<EOSY LunHel<EOSt<EOSHP*icHet<E2r RLu  \n",
      "----\n",
      "\n",
      "iter: 27000, p: 1350000, loss: 1.697653\n",
      "----\n",
      " c&Lin2)PscJwt<EliUxie FOA-Vot<EOrHo2X GHe<EOrig2 G \n",
      "----\n",
      "\n",
      "iter: 28000, p: 1400000, loss: 1.787015\n",
      "----\n",
      " attr<EJurt6Lid%in<ExitrJdsdeoRour GJClr/igWaOSUDer \n",
      "----\n",
      "\n",
      "iter: 29000, p: 1450000, loss: 2.079658\n",
      "----\n",
      " itirwk.MurJwsk 'lCeliliNoll%Mic<Elugg BuLie<EOtHed \n",
      "----\n",
      "\n",
      "iter: 30000, p: 1500000, loss: 2.248609\n",
      "----\n",
      " xo g<EOrHon. \n",
      "----\n",
      "\n",
      "iter: 31000, p: 1550000, loss: 1.858509\n",
      "----\n",
      " n4Fir-0YarisT;%5Whir BugNoreicGRerTiIcKesH.<EgrH<E \n",
      "----\n",
      "\n",
      "iter: 32000, p: 1600000, loss: 2.544990\n",
      "----\n",
      " uggis\tNorllize UOS>PurlIcJBurlLaLarlThury<EFugrbS. \n",
      "----\n",
      "\n",
      "iter: 33000, p: 1650000, loss: 2.695590\n",
      "----\n",
      " a re<EOSYo550%doioJrinZaPoP%on<EOSasUndgeocUnn<EOS \n",
      "----\n",
      "\n",
      "iter: 34000, p: 1700000, loss: 1.744839\n",
      "----\n",
      " ige KcAAbrHauc<EOSJBg% Dch EOtaEO'dSRoqurl<EOSHeJ- \n",
      "----\n",
      "\n",
      "iter: 35000, p: 1750000, loss: 1.683352\n",
      "----\n",
      "  Wa0XaTar<EOSY 2gap. Wa%t-Kazar0Gymar. GK%5-ZiegWa \n",
      "----\n",
      "\n",
      "iter: 36000, p: 1800000, loss: 1.841627\n",
      "----\n",
      " aIg5Y Latchlec<El<EluY LatMeregl Ka;%5Meruchtririr \n",
      "----\n",
      "\n",
      "iter: 37000, p: 1850000, loss: 1.797256\n",
      "----\n",
      " mateMBu OrbFGA%ieuug<EOSY g<EOc. ggJKcMeerMatNorDe \n",
      "----\n",
      "\n",
      "iter: 38000, p: 1900000, loss: 1.760848\n",
      "----\n",
      " mbug<EOS*SQ'dWate. Wal<EOSlk?'Wazoufir<EJuYssY<EOS \n",
      "----\n",
      "\n",
      "iter: 39000, p: 1950000, loss: 2.266480\n",
      "----\n",
      " wirin2AgY 0.<EKitTJwiS>PongrttPNoritZNoXiee<EOSUng \n",
      "----\n",
      "\n",
      "iter: 40000, p: 2000000, loss: 1.358909\n",
      "----\n",
      " al<EOSUngNolc<EOSHos50LaBe<EOSHoAXieg<EOSalG~lHeP. \n",
      "----\n",
      "\n",
      "iter: 41000, p: 2050000, loss: 1.230391\n",
      "----\n",
      " armat. Norma.faKy50-'FiHa.MagrawNor MazaMef>waja U \n",
      "----\n",
      "\n",
      "iter: 42000, p: 2100000, loss: 2.702576\n",
      "----\n",
      " armMi. Br%armpSHaeAmark<EOSHoN'dHea DarHaq;narmpRo \n",
      "----\n",
      "\n",
      "iter: 43000, p: 2150000, loss: 1.705420\n",
      "----\n",
      " al<EOSDu*Het<EOrHezXi05OS>slic<EOSUxiIcLeLeLetrtel \n",
      "----\n",
      "\n",
      "iter: 44000, p: 2200000, loss: 1.886705\n",
      "----\n",
      " ird FigKr UR-A%55UJirrigIA;rUn.chiot Ror Gi RoqunY \n",
      "----\n",
      "\n",
      "iter: 45000, p: 2250000, loss: 1.296421\n",
      "----\n",
      " nil PsAu]HWaAxalecI;K50-Jour I'AJrac.yck%PscTeoKRI \n",
      "----\n",
      "\n",
      "iter: 46000, p: 2300000, loss: 2.234812\n",
      "----\n",
      " war I-Ic%`rLFK2 we.o Ro F-Zoa>Bun<EOSaIcT25eufferI \n",
      "----\n",
      "\n",
      "iter: 47000, p: 2350000, loss: 1.227301\n",
      "----\n",
      " att0T. GraFWA;rUxpergukvyrePsJegiff'ngUn-ZfzektPs  \n",
      "----\n",
      "\n",
      "iter: 48000, p: 2400000, loss: 1.573371\n",
      "----\n",
      " eoge GhB2 Hel>ze5kLaurHetS>Yal<EOrgt<EFurl<EFostJo \n",
      "----\n",
      "\n",
      "iter: 49000, p: 2450000, loss: 1.622938\n",
      "----\n",
      " S>Fin<EOSY La FiloidurnPois EOSDgaugeordMe<EOSNid5 \n",
      "----\n",
      "\n",
      "iter: 50000, p: 2500000, loss: 2.764269\n",
      "----\n",
      "  SllM. Go WaAbdWavele<El WaBS>Nore<EOAY Cl<EloockT \n",
      "----\n",
      "\n",
      "iter: 51000, p: 2550000, loss: 1.648856\n",
      "----\n",
      "  Bu0Ma. onHyc=MfiFizer. GJrinJIxMCh--. eH'FqunHPNo \n",
      "----\n",
      "\n",
      "iter: 52000, p: 2600000, loss: 1.926467\n",
      "----\n",
      " OS>Ig50Nor MDzat. Wa%M5Ka. Norm Jret. GrJDric<EOSU \n",
      "----\n",
      "\n",
      "iter: 53000, p: 2650000, loss: 1.572797\n",
      "----\n",
      " S>IchTLa Bun<EOcecJ|Nor<EOcerg5MurGereer<EOSUrt'di \n",
      "----\n",
      "\n",
      "iter: 54000, p: 2700000, loss: 1.597669\n",
      "----\n",
      " ziS>Gr. 2Yyo.<EOSUr?'Ur<EOSit NoJir<EOSU U'dHif No \n",
      "----\n",
      "\n",
      "iter: 55000, p: 2750000, loss: 2.496604\n",
      "----\n",
      " >Trc<EOEDrgMe5e<EecUCAStic<ExcdrgIxManeDancMuGghtK \n",
      "----\n",
      "\n",
      "iter: 56000, p: 2800000, loss: 1.896284\n",
      "----\n",
      " as UnqunDas<EKaUrJwdo CJos Zlis'dic<EOSUch'dol<E.  \n",
      "----\n",
      "\n",
      "iter: 57000, p: 2850000, loss: 1.849360\n",
      "----\n",
      " nterTKyc<Ees<ElsLasFiree.XrnNormerSDe'de<ElterVisz \n",
      "----\n",
      "\n",
      "iter: 58000, p: 2900000, loss: 1.679875\n",
      "----\n",
      " stecJ. Atl. DoJeUn<EOrLoJInY<E. UnAJss<EOSUKr'dHon \n",
      "----\n",
      "\n",
      "iter: 59000, p: 2950000, loss: 1.265671\n",
      "----\n",
      " c<EKro. IcUKrJrr G. PsUncRotiG. lef<EldUnd-ja Hegy \n",
      "----\n",
      "\n",
      "iter: 60000, p: 3000000, loss: 1.623173\n",
      "----\n",
      " ard5Y. Gr Da\u000b",
      "F% Fi.icJol 'MBrnzlJNol<Ex Lid<EOseJs \n",
      "----\n",
      "\n",
      "iter: 61000, p: 3050000, loss: 2.021028\n",
      "----\n",
      " S>zaicHeKr. DwDuRocJingPDXanickXPsc<EOSUn Xdor<E.  \n",
      "----\n",
      "\n",
      "iter: 62000, p: 3100000, loss: 1.445132\n",
      "----\n",
      " ark5Y. DrTD2\u000b",
      ">2<EenLisIcHeG. RufNoAFiOS>AeUJ>J=Bu- \n",
      "----\n",
      "\n",
      "iter: 63000, p: 3150000, loss: 1.671872\n",
      "----\n",
      " reEYe<EOSUrJFloaOSIngur<ENoUldilFiOS>Kyor<EOnUrrIn \n",
      "----\n",
      "\n",
      "iter: 64000, p: 3200000, loss: 1.033022\n",
      "----\n",
      " OSize5ZSmjaumpS>AveryTy tT2 %pageAXeegg Cram DAgeo \n",
      "----\n",
      "\n",
      "iter: 65000, p: 3250000, loss: 2.487779\n",
      "----\n",
      " ic<EX NoJSpaeic<EEOSX 50%M.geun<E. UrftBe EOSD ES. \n",
      "----\n",
      "\n",
      "iter: 66000, p: 3300000, loss: 1.800126\n",
      "----\n",
      " air5J. Dd0%  >'HecTTYee<E. UJJigo >-za G. D'Ve<E.  \n",
      "----\n",
      "\n",
      "iter: 67000, p: 3350000, loss: 1.826127\n",
      "----\n",
      " argY CZ'd%'CaLiseHNoAmeHWaAbaHus'deach'Y ChT'Yeer< \n",
      "----\n",
      "\n",
      "iter: 68000, p: 3400000, loss: 2.473167\n",
      "----\n",
      " hic<E<E<E<E<E2s<ENo5Bug<EOSUnJ'dHougzeneWaOUnt<EY  \n",
      "----\n",
      "\n",
      "iter: 69000, p: 3450000, loss: 2.122921\n",
      "----\n",
      " arg. GixMaYaMLa . Noc<Eec<EOS5Ka. 5. IwsGriS>Yar<E \n",
      "----\n",
      "\n",
      "iter: 70000, p: 3500000, loss: 2.123119\n",
      "----\n",
      " iS>erCNoc<Een2'dUfik.embunzoniOSXssUJ<E. KiJNor<EO \n",
      "----\n",
      "\n",
      "iter: 71000, p: 3550000, loss: 1.279744\n",
      "----\n",
      " amMeng<Eec2'd505t. sictleca-<El<EX S>s'CoaZ<Erc<Eg \n",
      "----\n",
      "\n",
      "iter: 72000, p: 3600000, loss: 1.555456\n",
      "----\n",
      " n<E. GhJ505<EOSKeGTos<EOSUrTS%H<EOSUKpIlHond<EOSet \n",
      "----\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-50da1bdf716d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                                         feed_dict={inputs: input_vals,\n\u001b[1;32m     34\u001b[0m                                                    \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                                    init_state: h_t})\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# Progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/florian/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/florian/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1113\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/florian/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mdirect\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \"\"\"\n\u001b[0;32m--> 419\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/florian/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now that all the functions are set up we can run this thing\n",
    "\n",
    "# function to one hot encode the characters\n",
    "def one_hot(v):\n",
    "    return np.eye(vocab_size)[v]\n",
    "\n",
    "# Session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Initial values\n",
    "n, p = 0, 0\n",
    "#hprev_val = np.zeros([1, hidden_size])\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for pokemon in pokemons:\n",
    "        for c in range(len(pokemon)):\n",
    "            # Initialize the hidden state to 0 at the beginning of each sequence\n",
    "            h_t = np.zeros([1, hidden_size])\n",
    "\n",
    "            # Prepare inputs\n",
    "            input_vals, target_vals = list_iter(pokemon)\n",
    "            \n",
    "            input_vals = [char2idx[c] for c in input_vals]\n",
    "            target_vals = [char2idx[c] for c in target_vals]\n",
    "\n",
    "            input_vals  = one_hot(input_vals)\n",
    "            target_vals = one_hot(target_vals)\n",
    "   \n",
    "            # run the tensorflow session\n",
    "            h_t, loss_val, _ = sess.run([h_0, loss, updates],\n",
    "                                        feed_dict={inputs: input_vals,\n",
    "                                                   targets: target_vals,\n",
    "                                                   init_state: h_t})\n",
    "            if n % 1000 == 0:\n",
    "                # Progress\n",
    "                print('iter: %d, p: %d, loss: %f' % (n, p, loss_val))\n",
    "\n",
    "                # Do sampling\n",
    "                sample_length = 50\n",
    "                prime_str_idx = np.random.randint(len(string.ascii_uppercase))\n",
    "                prime_str = string.ascii_uppercase[prime_str_idx]\n",
    "                \n",
    "                idxs = []\n",
    "                sample_prev_state_val = np.copy(h_t[-1]).reshape(1,256)\n",
    "                sample_input_vals = one_hot([char2idx[prime_str]])\n",
    "\n",
    "                for t in range(sample_length):\n",
    "                    sample_output_softmax_val, sample_prev_state_val = \\\n",
    "                        sess.run([output_softmax, h_0],\n",
    "                                 feed_dict={inputs: sample_input_vals, init_state: sample_prev_state_val})\n",
    "                \n",
    "                    predicted_idx = (np.argmax(sample_output_softmax_val))\n",
    "                    \n",
    "                    idxs.append(predicted_idx)\n",
    "                    sample_input_vals = one_hot([predicted_idx])\n",
    "                \n",
    "                txt = ''.join(idx2char[ix] for ix in idxs)\n",
    "                print('----\\n %s \\n----\\n' % (txt.split('<EOS>')[0],))\n",
    "\n",
    "            p += seq_length\n",
    "            n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
