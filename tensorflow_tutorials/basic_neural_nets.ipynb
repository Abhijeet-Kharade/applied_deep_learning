{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "(17683, 9)\n",
      "(17683, 1)\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "# load the data into numpy arrays\n",
    "housing_features = housing.data\n",
    "housing_targets = housing.target\n",
    "\n",
    "n, f_dim = housing_features.shape\n",
    "\n",
    "# normalize the features\n",
    "housing_features= normalize(housing_features)\n",
    "\n",
    "# add bias\n",
    "housing_features = np.c_[np.ones((n, 1)), housing_features]\n",
    "\n",
    "housing_features = housing_features[housing_targets < 5]\n",
    "housing_targets = housing_targets[housing_targets < 5]\n",
    "\n",
    "# reshape targets to TF expectation\n",
    "housing_targets = np.expand_dims(housing_targets, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_features,\n",
    "                                                    housing_targets,\n",
    "                                                    test_size=0.1)\n",
    "\n",
    "print(housing.feature_names)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "EPOCHS = 5000\n",
    "n_inputs = 8\n",
    "n_hidden = 32\n",
    "n_output = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, f_dim+1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "def dense_layer(X, n_units, name, activation=None):\n",
    "    '''\n",
    "    Sets up a hidden layer to be used to build a multilayer perceptron. \n",
    "    \n",
    "    Initializes the weights of the neurons using a normal distribution with\n",
    "    standard deviation equal to 2 / sqrt(input_dimension + number_neurons)\n",
    "    \n",
    "    Parameters:\n",
    "    X: input data for the layer\n",
    "    n_units: number of neurons to use in the layer\n",
    "    name: the name of the scope to be used with this layer\n",
    "    activation: the tensorflow nonlinearity to be used for each neuron\n",
    "    \n",
    "    Returns:\n",
    "    Tensorflow graph description representing the constructed layer\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_units)\n",
    "        init = tf.truncated_normal((n_inputs, n_units), stddev=stddev)\n",
    "        W = tf.Variable(init, name='hidden_weights')\n",
    "        b = tf.Variable(tf.zeros([n_units]), name='bias')\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z\n",
    "        \n",
    "# build the network using the hidden layer function\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden = dense_layer(X, n_hidden, name='hidden_layer', activation=tf.nn.relu)\n",
    "    y_pred = dense_layer(hidden, 1, name='output')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    error = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.square(error), name='mse')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(LR).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Current loss: 6.57111 Test loss: 6.60319\n",
      "Epoch: 100 Current loss: 0.498092 Test loss: 0.490617\n",
      "Epoch: 200 Current loss: 0.427505 Test loss: 0.428701\n",
      "Epoch: 300 Current loss: 0.405509 Test loss: 0.412985\n",
      "Epoch: 400 Current loss: 0.394176 Test loss: 0.405215\n",
      "Epoch: 500 Current loss: 0.386787 Test loss: 0.399131\n",
      "Epoch: 600 Current loss: 0.381369 Test loss: 0.39438\n",
      "Epoch: 700 Current loss: 0.377212 Test loss: 0.390452\n",
      "Epoch: 800 Current loss: 0.373804 Test loss: 0.387079\n",
      "Epoch: 900 Current loss: 0.370866 Test loss: 0.384095\n",
      "Epoch: 1000 Current loss: 0.368259 Test loss: 0.381367\n",
      "Epoch: 1100 Current loss: 0.365885 Test loss: 0.378751\n",
      "Epoch: 1200 Current loss: 0.363673 Test loss: 0.376333\n",
      "Epoch: 1300 Current loss: 0.361594 Test loss: 0.374061\n",
      "Epoch: 1400 Current loss: 0.359628 Test loss: 0.371914\n",
      "Epoch: 1500 Current loss: 0.357778 Test loss: 0.369875\n",
      "Epoch: 1600 Current loss: 0.356018 Test loss: 0.367926\n",
      "Epoch: 1700 Current loss: 0.354331 Test loss: 0.365959\n",
      "Epoch: 1800 Current loss: 0.352719 Test loss: 0.36403\n",
      "Epoch: 1900 Current loss: 0.351204 Test loss: 0.362039\n",
      "Epoch: 2000 Current loss: 0.34975 Test loss: 0.360234\n",
      "Epoch: 2100 Current loss: 0.348342 Test loss: 0.358501\n",
      "Epoch: 2200 Current loss: 0.346974 Test loss: 0.356806\n",
      "Epoch: 2300 Current loss: 0.345622 Test loss: 0.35518\n",
      "Epoch: 2400 Current loss: 0.344309 Test loss: 0.353577\n",
      "Epoch: 2500 Current loss: 0.343035 Test loss: 0.351999\n",
      "Epoch: 2600 Current loss: 0.341638 Test loss: 0.350642\n",
      "Epoch: 2700 Current loss: 0.340289 Test loss: 0.349256\n",
      "Epoch: 2800 Current loss: 0.338941 Test loss: 0.347939\n",
      "Epoch: 2900 Current loss: 0.337629 Test loss: 0.346264\n",
      "Epoch: 3000 Current loss: 0.336376 Test loss: 0.344623\n",
      "Epoch: 3100 Current loss: 0.3351 Test loss: 0.343003\n",
      "Epoch: 3200 Current loss: 0.333826 Test loss: 0.341381\n",
      "Epoch: 3300 Current loss: 0.332579 Test loss: 0.339816\n",
      "Epoch: 3400 Current loss: 0.331361 Test loss: 0.338285\n",
      "Epoch: 3500 Current loss: 0.330181 Test loss: 0.33679\n",
      "Epoch: 3600 Current loss: 0.329038 Test loss: 0.335357\n",
      "Epoch: 3700 Current loss: 0.32793 Test loss: 0.333996\n",
      "Epoch: 3800 Current loss: 0.326858 Test loss: 0.332773\n",
      "Epoch: 3900 Current loss: 0.325814 Test loss: 0.331636\n",
      "Epoch: 4000 Current loss: 0.324767 Test loss: 0.330546\n",
      "Epoch: 4100 Current loss: 0.323747 Test loss: 0.329479\n",
      "Epoch: 4200 Current loss: 0.322748 Test loss: 0.328433\n",
      "Epoch: 4300 Current loss: 0.321778 Test loss: 0.327418\n",
      "Epoch: 4400 Current loss: 0.320842 Test loss: 0.326431\n",
      "Epoch: 4500 Current loss: 0.319927 Test loss: 0.32548\n",
      "Epoch: 4600 Current loss: 0.319024 Test loss: 0.32455\n",
      "Epoch: 4700 Current loss: 0.31813 Test loss: 0.323657\n",
      "Epoch: 4800 Current loss: 0.317241 Test loss: 0.322798\n",
      "Epoch: 4900 Current loss: 0.316366 Test loss: 0.321938\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "losses = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for e in range(EPOCHS):\n",
    "        if e % 100 == 0:\n",
    "            print(\"Epoch:\", e, \n",
    "                  \"Current loss:\", \n",
    "                  sess.run(loss, feed_dict={X: X_train, y: y_train}), \n",
    "                  \"Test loss:\",\n",
    "                 sess.run(loss, feed_dict={X: X_test, y: y_test}))\n",
    "            \n",
    "        sess.run(train_step, feed_dict={X: X_train, y: y_train})\n",
    "        losses.append(sess.run(loss, feed_dict={X: X_train, y: y_train}))\n",
    "        \n",
    "    save_path = saver.save(sess, '/tmp/mlp_regression.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
