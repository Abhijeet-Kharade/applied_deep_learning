{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Neural Networks with TF\n",
    "\n",
    "Constructing a neural network in TF low level python API allows you to create your own layers as functions to be combined in flexible ways. \n",
    "\n",
    "\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view     ...      grade  sqft_above  \\\n",
       "0      5650     1.0           0     0     ...          7        1180   \n",
       "1      7242     2.0           0     0     ...          7        2170   \n",
       "2     10000     1.0           0     0     ...          6         770   \n",
       "3      5000     1.0           0     0     ...          7        1050   \n",
       "4      8080     1.0           0     0     ...          8        1680   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0              0      1955             0    98178  47.5112 -122.257   \n",
       "1            400      1951          1991    98125  47.7210 -122.319   \n",
       "2              0      1933             0    98028  47.7379 -122.233   \n",
       "3            910      1965             0    98136  47.5208 -122.393   \n",
       "4              0      1987             0    98074  47.6168 -122.045   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           1340        5650  \n",
       "1           1690        7639  \n",
       "2           2720        8062  \n",
       "3           1360        5000  \n",
       "4           1800        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_dat = pd.read_csv('Housing_data/kc_house_data.csv')\n",
    "house_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19451, 18)\n",
      "(19451, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the data into numpy arrays\n",
    "housing_features = house_dat[house_dat.columns.difference(['id', 'price', 'date', 'zipcode'])].values\n",
    "housing_targets = house_dat.price.values\n",
    "housing_targets = housing_targets / np.max(housing_targets)\n",
    "\n",
    "n, f_dim = housing_features.shape\n",
    "\n",
    "# normalize the features\n",
    "housing_features= normalize(housing_features)\n",
    "\n",
    "# add bias\n",
    "housing_features = np.c_[np.ones((n, 1)), housing_features]\n",
    "\n",
    "# reshape targets to TF expectation\n",
    "housing_targets = np.expand_dims(housing_targets, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_features,\n",
    "                                                    housing_targets,\n",
    "                                                    test_size=0.1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Neural Network Hyperparameters\n",
    "\n",
    "LR, EPOCHS, BATCHSIZE, and N_HIDDEN are all parameters that you can play with to acheive the best performance.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "EPOCHS = 200\n",
    "BATCHSIZE = 32\n",
    "N_HIDDEN = 32\n",
    "N_OUTPUT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be training this network with mini-batch gradient descent we need to set up tensorflow placeholders for our input and target data. The place holders serve as buckets in our graph that we can place data into during training. We need only to define the shape of the data with respect to the feature dimensions. The `None` serves as a wildcard for the batchsize, having `None` there allows us to use any batch size that we choose during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, f_dim+1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Layers\n",
    "\n",
    "When using one of the popular high level libraries for deep learning they typically frame the process as stacking a bunch of layers together. The layers are often set up in a way that allows you to chain an arbitrary number together. The workflow with TensorFlow is similar except that we will create our own layer functions rather than importing them from some library. \n",
    "\n",
    "Fun fact, a lot of these popular high level deep learning libraries that are built of existing frameworks started because people were building and reusing these layers functions. Some decided to release their collections of these functions as a package. A few ended up getting popular and development accelerated. \n",
    "\n",
    "When designing our layer functions there are a few things to consider. At the minimum, we should think about what the data going into our layer will be, how many neurons to have in each of our layers, what kind of activation function to use, and maybe how we want our weights to be initialized at the start of training. When using TensorFlow it's also a good idea to have names for all of our scopes within the graph. When we have to trouble shoot it will use that name in the traceback and if we view the graph in some of the TensorFlow visualization tools. \n",
    "\n",
    "In the previous notebook we set up a regression problem that we vectorized by doing a simple vector to matrix multiply. We can make that case that this is the simplest form of a neural network, in which case we should only have to perform some minor changes to create a neural network layer. What we don't want to do is a put a bunch of these single neuron functions together in parallel, this would be terribly inefficient. Instead, we want to frame our calculations as matrix operations. If we think back to that example, our weight matrix was set up with shape `[feature_dimensions, 1]` where feature_dimensions corresponds to however many columns we had in our dataset. The 1 however, we can think of as how many neurons we had in that \"layer\". So if we want 5 neurons in a layer we can change the shape of that matrix to `[feature_dimensions, 5]`. This will give us a vector of length 5 for every example that we've run through the network. \n",
    "\n",
    "```python\n",
    "def dense_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        W = tf.Variable(tf.ones([n_inputs, 5]), name='hidden_weights')\n",
    "        Z = tf.matmul(X, W)\n",
    "        out = activation(Z)\n",
    "        return out\n",
    "```\n",
    "\n",
    "Additionally, we may want to initialize our weights to something other than a matrix of ones. One popular method is to use the [He normal initialization method (https://arxiv.org/pdf/1502.01852.pdf). We can set this up quickly with two more lines and then pass the resulting `init` matrix into our `tf.Variable` weight matrix instead of starting it with `tf.ones()`.\n",
    "\n",
    "```python\n",
    "stddev = 2 / np.sqrt(n_inputs + n_units)\n",
    "init = tf.truncated_normal((n_inputs, n_units), stddev=stddev)\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Build your own generic dense layer function. Note: we may want to make the activation functions optional. If we are doing a regression problem our final layer should not have an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dense_layer(X, n_units, name, activation=None):\n",
    "    '''\n",
    "    Sets up a hidden layer to be used to build a multilayer perceptron. \n",
    "    \n",
    "    Initializes the weights of the neurons using a normal distribution withd\n",
    "    standard deviation equal to 2 / sqrt(input_dimension + number_neurons)\n",
    "    \n",
    "    Parameters:\n",
    "    X: input data for the layer\n",
    "    n_units: number of neurons to use in the layer\n",
    "    name: the name of the scope to be used with this layer\n",
    "    activation: the tensorflow nonlinearity to be used for each neuron\n",
    "    \n",
    "    Returns:\n",
    "    Tensorflow graph description representing the constructed layer\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        # fill in the rest below\n",
    "        print('this will be a neural net layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_layer(X, n_units, name, activation=None):\n",
    "    '''\n",
    "    Sets up a hidden layer to be used to build a multilayer perceptron. \n",
    "    \n",
    "    Initializes the weights of the neurons using a normal distribution with\n",
    "    standard deviation equal to 2 / sqrt(input_dimension + number_neurons)\n",
    "    \n",
    "    Parameters:\n",
    "    X: input data for the layer\n",
    "    n_units: number of neurons to use in the layer\n",
    "    name: the name of the scope to be used with this layer\n",
    "    activation: the tensorflow nonlinearity to be used for each neuron\n",
    "    \n",
    "    Returns:\n",
    "    Tensorflow graph description representing the constructed layer\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_units)\n",
    "        init = tf.truncated_normal((n_inputs, n_units), stddev=stddev)\n",
    "        W = tf.Variable(init, name='hidden_weights')\n",
    "        b = tf.Variable(tf.zeros([n_units]), name='bias')\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a neural network from this layer definition is fairly straight forward now. We essentially just create a new named scope and stack two of our newly created layers together with the desired parameters. If we wanted more layers we would simply just add more to the center of our network definition.\n",
    "\n",
    "We will also create TF scopes for the loss and SGD functions, a slight twist on what we did before but the functionality is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the network using the hidden layer function\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden = dense_layer(X, N_HIDDEN, name='hidden_layer', activation=tf.nn.relu)\n",
    "    y_pred = dense_layer(hidden, N_OUTPUT, name='output')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    error = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.square(error), name='mse')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(LR).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network using mini-batch gradient descent. We will slice up our training data into batches of size `BATCHSIZE` and feed each batch into the network in succession. Just like we did with the linear model in the previous notebook, but with smaller portions of the dataset for each step rather than the whole thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Current loss: 0.013016395 Test loss: 0.017964363\n",
      "Epoch: 10 Current loss: 0.0006867899 Test loss: 0.0011087939\n",
      "Epoch: 20 Current loss: 0.00046643557 Test loss: 0.00073263195\n",
      "Epoch: 30 Current loss: 0.00041283088 Test loss: 0.00065215194\n",
      "Epoch: 40 Current loss: 0.0003834699 Test loss: 0.00061598007\n",
      "Epoch: 50 Current loss: 0.0003608526 Test loss: 0.0005976683\n",
      "Epoch: 60 Current loss: 0.00034316143 Test loss: 0.00058545696\n",
      "Epoch: 70 Current loss: 0.00032912748 Test loss: 0.0005760594\n",
      "Epoch: 80 Current loss: 0.0003184687 Test loss: 0.00056864927\n",
      "Epoch: 90 Current loss: 0.0003092282 Test loss: 0.00056271144\n",
      "Epoch: 100 Current loss: 0.0003018648 Test loss: 0.0005572072\n",
      "Epoch: 110 Current loss: 0.00029545178 Test loss: 0.0005521858\n",
      "Epoch: 120 Current loss: 0.00028960718 Test loss: 0.00054785964\n",
      "Epoch: 130 Current loss: 0.00028453933 Test loss: 0.0005435186\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "losses = []\n",
    "n_samples = X_train.shape[0]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for e in range(EPOCHS):\n",
    "            \n",
    "        for i in range((n_samples + BATCHSIZE - 1) // BATCHSIZE):\n",
    "            sl = slice(i * BATCHSIZE, (i+1) * BATCHSIZE)\n",
    "            X_b = X_train[sl]\n",
    "            y_b = y_train[sl]\n",
    "            _, train_loss = sess.run([train_step, loss], feed_dict={X: X_b, y: y_b})\n",
    "            losses.append(train_loss)\n",
    "            \n",
    "        if e % 10 == 0:\n",
    "            print(\"Epoch:\", e, \n",
    "                  \"Current loss:\", train_loss, \n",
    "                  \"Test loss:\",\n",
    "                  sess.run(loss, feed_dict={X: X_test, y: y_test}))\n",
    "        \n",
    "    save_path = saver.save(sess, '/tmp/mlp_regression.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load the saved model and make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# best weight values are saved in the checkpoint\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '/tmp/mlp_regression.ckpt')\n",
    "    preds = sess.run(y_pred, feed_dict={X: X_test})\n",
    "    print(sess.run(loss, feed_dict={X: X_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did better than the simple linear regression from the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, preds)\n",
    "ax.plot([y_test.min(), y_test.max()], \n",
    "        [y_test.min(), y_test.max()], '--', lw=3, color='r')\n",
    "ax.set_xlabel('True Value')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
